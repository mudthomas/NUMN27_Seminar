\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath, amsthm}
\usepackage{python}
\usepackage{listings}
\usepackage{float}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{nccmath}
\usepackage{cite}
\usepackage{hyperref}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
% % \theoremstyle{definition} %Makes def,ex and remark without italics
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}


\newcommand{\quoteswe}[1]{”#1”}
\newcommand{\quoteeng}[1]{“#1”}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\inner}[2]{\left< #1 , #2 \right>}
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}
% \renewcommand{\phi}{\varphi}
% \renewcommand{\Phi}{\varPhi}
\newcommand{\sgn}[1]{\operatorname{sgn} (#1)}
\newcommand{\csgn}[1]{\operatorname{csgn} (#1)}
\newcommand{\argmin}[1]{\underset{#1}{\arg\min}}
\newcommand{\Arg}{\operatorname{Arg}}
\renewcommand{\exp}[1]{\operatorname{e}^{#1}}
\newcommand{\expalt}{\operatorname{exp}}
\renewcommand{\i}{\mathrm{i}}
\newcommand{\id}{\mathrm{I}}
\newcommand{\T}{\mathrm{T}}
\newcommand{\coi}[2]{\left[#1, #2\right]}


\title{Relaxation Runge-Kutta Methods and Entropy Stability}
% \subtitle{A presentation of an article by Ranocha et al.}
\author{Thomas Renström}

\date{\today}

\begin{document}
\maketitle
\section{Introduction}

For certain simulations it is of utmost importance to use methods that follow conservation laws, making sure that the model stays true to physical laws. While there are already established methods for this, they are not as well known nor as simple to set up as the Runge-Kutta method. We will present the idea put forward by Ranocha et al. in \cite{Ranocha_2020} and attempt to replicate their result. We shall see that not only is the modification suggested in the article by both simple and cheap, it is also accurate.
As this report is based on \cite{Ranocha_2020} we concern ourself mainly with the statements made there. A few of their references, those deemed as significant by this author, will be repeated here.

\section{Preliminaries}
\subsection{ODE}
    We consider a time-dependent ODE of the form
    \begin{align*}
        \frac{\text{d}}{\text{d}t} u(t) &= f(t, u(t)), \quad t \in (0, T)\\
        u(0) &= u^0
    \end{align*}
    for \(u \in \mathcal{H}\) being a real Hilbert space with the inner product \(\inner{\cdot}{\cdot}\), inducing the norm \(\norm{\cdot}\).


\subsection{Entropy}
    We denote by
    \[ \eta: \mathcal{H} \rightarrow \mathbb{R}\]
    a smooth convex function in time.
    We call this entropy, while in other application it might instead represent some other form of non-increasing quantity, e.g. energy or momentum.

    The change in entropy over time is given by
    \[\frac{\text{d}}{\text{d}t} \eta (u(t)) = \inner{\eta'(u(t))}{f(t, u(t))}.\]

    \vspace*{5mm}
    A entropy dissipative system will satisfy
    \[\inner{\eta'(u(t))}{f(t, u(t))} \leq 0, \quad \forall u \in \mathcal{H}, t \in [0, T],\]
    and a entropy conservative system will satisfy
    \[\inner{\eta'(u(t))}{f(t, u(t))} = 0, \quad \forall u \in \mathcal{H}, t \in [0, T].\]


\subsection{Classic Runge-Kutta}
    A general Runge-Kutta method with \(s\) stages is represented with the Butcher tableau
    \[\begin{array}{c|c}
        c & A\\
        \hline
        ~ & b^{\T}
    \end{array},\]
    with \(A \in \mathbb{R}^{s \times s}\) and \(b, c \in \mathbb{R}^s.\)

    \vspace*{7mm}
    \noindent
    The scheme of the method is
    \begin{align}
        y_i &= u^{n} + \Delta t\sum_{j=1}^{s} a_{i,j} f(t_n + c_j \Delta t, y_j), \quad i = 1,\ldots,s \label{Eq_RK_Stage} \\
        u^{n+1} &= u^{n} + \Delta t \sum_{i=1}^{s} b_i f(t_n + c_i \Delta t, y_i). \label{Eq_RK_Scheme}
    \end{align}
    where \(y_i\) are the stage values.

    \vspace*{7mm}
    \noindent
    For brevity we will use the notation
    \[f_i := f(t_n + c_i \Delta t, y_i), \qquad f_0 := f(t_n, u^n).\]


\section{Relaxation Runge-Kutta Methods}
\subsection{Relaxation Runge-Kutta}
    The basic idea of the Relaxed Runge-Kutta method is to introduce a scaling factor to the weights \(b_i\). We call this scaling factor \(\gamma_n \in \mathbb{R}\) and construct a new scheme
    \[u^{n+1}_{\gamma} = u^{n} + \gamma_n \Delta t \sum_{i=1}^{s} b_i f_i,\]
    with \(\gamma_n \in \mathbb{R}\).

    \vspace*{10mm}
    Relaxed Runge-Kutta, RRK, interprets \(u_{\gamma}^{n+1} \approx u(t_n + \gamma \Delta t)\). This means that using this method we will not be using a uniform step length and the number of steps might not be the same as with the regular RK method.

    \vspace*{5mm}
    The incremental direction technique, or IDT-method, interprets \(u_{\gamma}^{n+1} \approx u(t_n + \Delta t)\). This method retains the original step length, and takes as many steps to complete as the regular RK method it is based on.

    \vspace*{5mm}
    In~\cite{ketcheson2019relaxation} it was proposed choosing \(\gamma_n\) such that
    \[ \frac{\norm{u^{n+1}_{\gamma}}^2 - \norm{u^{n}}^2}{2} = \gamma_n \Delta t \sum_{i=1}^{s} b_i \inner{y_i}{f_i} .\]

    In this article however the suggestion is to instead use \(\gamma_n\) fulfilling the condition
    \[  \eta (u^{n+1}_{\gamma}) - \eta (u^{n}) = \gamma \Delta t \sum_{i=1}^{s} b_i \inner{\eta'(y_i)}{f_i}.\]
    This is done by finding a root of
    \begin{equation}\label{Eq_r}
        r(\gamma) = \eta (u^{n} + \gamma_n \Delta t \sum_{i=1}^{s} b_i f_i) - \eta (u^{n}) - \gamma \Delta t \sum_{i=1}^{s} b_i \inner{\eta'(y_i)}{f_i}.
    \end{equation}

    The direction and entropy change
    \begin{align*}
        d^n &:= \sum^s_{i=1}b_if_i \\
        e   &:= \Delta t \sum^s_{i=1} b_i \inner{\eta'(y_i)}{f_i}
    \end{align*}
    can both be computed on the fly during the RK method. This reduces finding the root of \(r\) to just a scalar root finding problem.

    Note that since \(\eta\) is convex we have that \(r\) is convex in \(\gamma\).

    We note also that since
    \begin{equation}\label{Eq_r_at_0}
        r(0) = \eta (u^{n} + 0) - \eta (u^{n}) - 0 = 0
    \end{equation}
    \(r\) has a root at \(0\).

    Using a non-positive \(\gamma\) would not be feasible. A zero-valued \(\gamma\) would halt the scheme, while a negative \(\gamma\) would amount to steps backwards in time.

% \subsection{title}
%     Theorem 2.2
%

% \subsection{Existence of a solution}
% \subsection{Existence of a solution}
%     We know that \(r\) has one of the following forms
%     PICTURE
%     Now, in order to prove that \(r\) has a positive root we show that \(r(\gamma)\) is negative for small enough \(\gamma > 0\) and positive for large \(\gamma > 0\).
%


\subsection{Existence of a solution}
In order to show that \(r\) has a positive root we need two things. Firstly that \(r'\) is negative at the root \(\gamma = 0\) and that \(r'\) is positive at some point \(\gamma > 0\).

    \begin{lemma}
        Let a Runge-Kutta method be given such that \(\sum_{i=1}^{s}b_{i}a_{i,j}>0\). If \(n''(u^n)(f_0,f_0) > 0\) then \(r'(0)<0\) for sufficiently small \(\Delta t > 0\).
    \end{lemma}

    \vspace*{5mm}
    Note that \(\sum_{i=1}^{s}b_{i}a_{i,j}>0\) is a reasonable assumption,
    since \(\sum_{i=1}^{s}b_{i}a_{i,j}=1/2\) is a condition for second-order accuracy.

    % \begin{proof}
    %     By definition of \(r\) we have that
    %     \begin{align*}
    %         r'(0)   &= \eta'(u^n) \cdot \left(\Delta t \sum_{i=1}^{s}b_if_i\right) - \Delta t \sum_{i=1}^{s}b_i\inner{\eta'(y_i)}{f_i}\\
    %         &= -\Delta t^2 \sum_{i,j=1}^{s}b_ia_{i,j} \int_{0}^{1}\eta''\left(u^n + v\Delta t \sum_{k=1}^{s}a_{i,k}f_k\right)\left(f_i,f_j\right)\text{d}v.
    %     \end{align*}
    %     With Taylor expansions of \(f_i, f_j = f_0 + \mathcal{O}(\Delta t)\),
    %     \begin{align*}
    %         r'(0) &= -\Delta t^2 \sum_{i,j=1}^{s}b_ia_{i,j} \int_{0}^{1}\eta''\left(u^n + v\Delta t \sum_{k=1}^{s}a_{i,k}f_k\right)\left(f_0,f_0 \right)\text{d}v + \mathcal{O}(\Delta t^3)
    %     \end{align*}
    %     Thus, with the given assumptions, \(r'(0)<0\).
    % \end{proof}

    \begin{proof}
        By the definition of \(r(\gamma)\) in Equation~\ref{Eq_r},
        \begin{align*}
            \frac{dr}{d\gamma}  &= \eta'(u^n + \gamma \Delta t \sum_{i=1}^{s}b_if_i) \cdot \left(\Delta t \sum_{i=1}^{s}b_if_i\right) - \Delta t \sum_{i=1}^{s}b_i\inner{\eta'(y_i)}{f_i}.
        \end{align*}

        Evaluating \(r'(\gamma)\) at \(\gamma = 0\) we get
        \begin{align*}
            r'(0)   &= \eta'(u^n) \cdot \left(\Delta t \sum_{i=1}^{s}b_if_i\right) - \Delta t \sum_{i=1}^{s}b_i\inner{\eta'(y_i)}{f_i}\\
                    &= \Delta t \sum_{i=1}^{s}b_i \left( \inner{\eta'(u^n)}{f_i} - \inner{\eta'(y_i)}{f_i}\right) \\
                    &= -\Delta t \sum_{i=1}^{s}b_i \left( \inner{\eta'(y_i)}{f_i} - \inner{\eta'(u^n)}{f_i}\right).
        \end{align*}
        We expand \(y_i = u^n + \Delta t \sum_{j=1}^{s}a_{i,j}f_j\) as defined in Equation~\ref{Eq_RK_Stage} and get
        \begin{align*}
            r'(0) &= -\Delta t \sum_{i=1}^{s}b_i \left( \inner{\eta'\left(u^n + \Delta t \sum_{j=1}^{s}a_{i,j}f_j\right)}{f_i} - \inner{\eta'(u^n)}{f_i}\right).
        \end{align*}


        Then, by the fundamental theorem of calculus,
        \begin{align*}
            r'(0)   &= -\Delta t \sum_{i=1}^{s}b_i \int_{0}^{1}\eta''\left(u^n + v\Delta t \sum_{k=1}^{s}a_{i,k}f_k\right)\left(f_i,\Delta t \sum_{j=1}^{s}a_{i,j}f_j\right)\text{d}v \\
%                    &= -\Delta t^2 \sum_{i=1}^{s}b_i \int_{0}^{1}\eta''\left(u^n + \Delta t \sum_{k=1}^{s}a_{i,k}f_k\right)\left(f_i,\sum_{j=1}^{s}a_{i,j}f_j\right)\text{d}v \\
                    &= -\Delta t^2 \sum_{i,j=1}^{s}b_ia_{i,j} \int_{0}^{1}\eta''\left(u^n + v\Delta t \sum_{k=1}^{s}a_{i,k}f_k\right)\left(f_i,f_j\right)\text{d}v.
        \end{align*}
        With Taylor expansions of \(f_i, f_j = f_0 + \mathcal{O}(\Delta t)\),
        \begin{align*}
            r'(0)   &= -\Delta t^2 \sum_{i,j=1}^{s}b_ia_{i,j} \int_{0}^{1}\eta''\left(u^n + v\Delta t \sum_{k=1}^{s}a_{i,k}f_k\right)\left(f_0,f_0 + \mathcal{O}(\Delta t)\right)\text{d}v\\
                    &= -\Delta t^2 \sum_{i,j=1}^{s}b_ia_{i,j} \int_{0}^{1}\eta''\left(u^n + v\Delta t \sum_{k=1}^{s}a_{i,k}f_k\right)\left(f_0,f_0 \right)\text{d}v + \mathcal{O}(\Delta t^3)
        \end{align*}
        For sufficiently small \(\Delta t\) we have that the integral is positive since \(\eta''(u^n)(f_0,f_0) > 0\) and thus, with the other assumptions, \(r'(0)<0\).
    \end{proof}


    \begin{lemma}\label{Lem_rp1}
        Let a Runge-Kutta method be given such that \(\sum_{i,j=1}^{s}b_{i}(a_{i,j} - b_j)<0\). If \(\eta''(u^n)(f_0,f_0) > 0\) then \(r'(1)>0\) for sufficiently small \(\Delta t > 0\).
    \end{lemma}

    \vspace*{5mm}
    Note that the assumption \(\sum_{i,j=1}^{s}b_{i}(a_{i,j} - b_j)<0\) is reasonable. Since \(\sum_{i,j=1}^{s}b_{i}b_j = 1\) and \(\sum_{i=1}^{s}b_{i}a_{i,j}=1/2\) is a condition for second-order accuracy we have that \(\sum_{i,j=1}^{s}b_{i}(a_{i,j} - b_j)=-1/2\)

    \begin{proof}
        By the definition of \(r(\gamma)\) in Equation~\ref{Eq_r},
        \begin{align*}
            \frac{dr}{d\gamma}  &= \eta'(u^n + \gamma \Delta t \sum_{i=1}^{s}b_if_i) \cdot \left(\Delta t \sum_{i=1}^{s}b_if_i\right) - \Delta t \sum_{i=1}^{s}b_i\inner{\eta'(y_i)}{f_i}.
        \end{align*}

        Evaluating \(r'(\gamma)\) at \(\gamma = 1\) we get
        \begin{align*}
            r'(1)   &= \eta'(u^n + \Delta t \sum_{j=1}^{s}b_jf_j) \cdot \left(\Delta t \sum_{i=1}^{s}b_if_i\right) - \Delta t \sum_{i=1}^{s}b_i\inner{\eta'(y_i)}{f_i} \\
                    &= \Delta t \sum_{i=1}^{s}b_i \left( \inner{\eta'(u^n + \Delta t \sum_{j=1}^{s}b_jf_j)}{f_i} - \inner{\eta'(y_i)}{f_i} \right) \\
                    &= -\Delta t \sum_{i=1}^{s}b_i \left( \inner{\eta'(y_i)}{f_i} - \inner{\eta'(u^n + \Delta t \sum_{j=1}^{s}b_jf_j)}{f_i} \right)
        \end{align*}

        Expanding \(y_i = u^n + \Delta t \sum_{j=1}^{s}a_{i,j}f_j\) as defined in Equation~\ref{Eq_RK_Stage} we get
        \begin{equation*}
            r'(1) = -\Delta t \sum_{i=1}^{s}b_i \left( \inner{\eta'(u^n + \Delta t \sum_{j=1}^{s}a_{i,j}f_j)}{f_i} - \inner{\eta'(u^n + \Delta t \sum_{j=1}^{s}b_jf_j)}{f_i} \right).
        \end{equation*}

        We then make substitutions according to Equation~\ref{Eq_RK_Scheme}, \(u^{n+1} = u^n + \Delta t \sum_{j=1}^{s}b_jf_j\),
        \begin{equation*}
            r'(1) = -\Delta t \sum_{i=1}^{s}b_i \left(\inner{\eta'\left(u^{n+1} + \Delta t \sum_{j=1}^s \left(a_{i,j}-b_j\right)f_j\right)}{f_i} - \inner{\eta'(u^{n+1})}{f_i} \right).
        \end{equation*}

        Then, by the fundamental theorem of calculus,
        \begin{equation*}
            r'(1) = -\Delta t \sum_{i=1}^{s}b_i \int_{0}^{1} \eta'' \left(u^{n+1} + \Delta t \sum_{k=1}^s \left(a_{i,k}-b_k\right)f_k\right)\left(f_i,\Delta t \sum_{j=1}^s \left(a_{i,j}-b_j\right)f_j\right) \text{d}v
        \end{equation*}
        \begin{equation*}
            r'(1) = -\Delta t^2 \sum_{i,j=1}^{s}b_i\left(a_{i,j}-b_j\right) \int_{0}^{1} \eta'' \left(u^{n+1} + \Delta t \sum_{k=1}^s \left(a_{i,k}-b_k\right)f_k\right)\left(f_i,f_j\right) \text{d}v
        \end{equation*}

        With Taylor expansions of \(f_i, f_j = f_0 + \mathcal{O}(\Delta t)\),
        \begin{equation*}
            r'(1) = -\Delta t^2 \sum_{i,j=1}^{s}b_i\left(a_{i,j}-b_j\right) \int_{0}^{1} \eta'' \left(u^{n+1} + \Delta t \sum_{k=1}^s \left(a_{i,k}-b_k\right)f_k\right) \left(f_0,f_0+\mathcal{O}(\Delta t)\right) \text{d}v
        \end{equation*}
        \begin{equation*}
            r'(1) = -\Delta t^2 \sum_{i,j=1}^{s}b_i\left(a_{i,j}-b_j\right) \int_{0}^{1} \eta'' \left(u^{n+1} + \Delta t \sum_{k=1}^s \left(a_{i,k}-b_k\right)f_k\right) \left(f_0,f_0\right) \text{d}v+\mathcal{O}(\Delta t^3)
        \end{equation*}
        For sufficiently small \(\Delta t\) we have that the integral is positive since \(\eta''(u^n)(f_0,f_0) > 0\) and thus with the other assumptions, \(r'(1)>0\).
    \end{proof}

    \begin{theorem}
        Assume that the Runge-Kutta method satisfies \(\sum_{i,j=1}^{s} b_ia_{i,j}> 0\) and \(\sum_{i,j=1}^{s} b_i(a_{i,j}-b_j) < 0\). If \(\eta''(u^n)(f_0,f_0) > 0\) then r has a positive root for sufficiently small \(\Delta t > 0\).
    \end{theorem}
    \begin{proof}
        Since \(r(0)=0\) and \(r'(0) < 0\) we have that \(r(\gamma)<0\) for small \(\gamma > 0\). Because \(r'(1)>0\) and \(r\) is convex we have that \(r'\) is monotone. Thus, there must be a positive root of \(r\).
    \end{proof}


\subsection{Accuracy}
    The following theorem is presented in \cite{Ranocha_2020} without a proof with a reference to \cite{ketcheson2019relaxation}. We will proceed, accepting it at face value.

    \begin{theorem}\label{Th_Ketcheson}
        Let a given RK-method be of order \(p\). Consider the IDT and RRK methods based on them and suppose that \(\gamma_n = 1 + \mathcal{O}(\Delta t^{p-1})\), then
        \begin{enumerate}
            \item The IDT method interpreting \(u_{\gamma}^{n+1} \approx u(t_n + \Delta t)\) has order \(p-1\).
            \item The RRK method interpreting \(u_{\gamma}^{n+1} \approx u(t_n + \gamma \Delta t)\) has order \(p\).
        \end{enumerate}
    \end{theorem}

    \vspace*{3mm}
    \noindent
    Thus, if we can show that \(\gamma_n = 1 + \mathcal{O}(\Delta t^{p-1})\), then our method is accurate.

    \vspace*{5mm}
    Setting \(u^n = u(t_n)\) a Runge-Kutta method of order \(p\) gives
    \begin{align}
        \eta(u^{n+1})-\eta(u^n) &= \eta(u(t_n + \Delta t)) - \eta(u^n) + \mathcal{O}(\Delta t^{p+1}) \nonumber \\
        &= \int_{t_n}^{t_n + \Delta t} \inner{\eta'(u(t))}{f(t, u(t))}\text{d}t + \mathcal{O}(\Delta t^{p+1}) \nonumber \\
        &= \Delta t \sum_{i=1}^{s} \inner{\eta'(u(t_n + c_i\Delta t))}{f(t_n+c_i\Delta t,u(t_n+c_i\Delta t))} + \mathcal{O}(\Delta t^{p+1}). \label{Eq_for_Corr}
    \end{align}
    Furthermore, we have by the local truncation error of a Runge-Kutta method of order \(p\) that
    \[ \sum_{i=1}^{s}b_if(t_n+c_i\Delta t, y_i) = \sum_{i=1}^{s}b_if(t_n + c_i\Delta t, u(t_n + c_i \Delta t)) + \mathcal{O}(\Delta t^p),\] and we know that the elements of the Butcher's tableau pertaining to a Runge-Kutta method is not dependent on \(f\) we test whether we can replace \(f\) by any smooth function.

    \begin{theorem}\label{Th_psi}
        Let \(\mathcal{W}\) be a Banach space, \(\psi: \left[0,T\right]\times \mathcal{H} \rightarrow \mathcal{W}\) a smooth function and \(b_i, c_i\) coefficients of a Runge-Kutta method of order \(p\). Then
        \[\sum_{i=1}^s b_i\psi(t_n + c_i \Delta t, y_i) = \sum_{i=1}^{s}b_i\psi(t_n + c_i \Delta t, u(t_n + c_i \Delta t)) + \mathcal{O}(\Delta t^p).\]
    \end{theorem}
    \begin{proof}
        We consider the function \(\phi = \int_{t_n}^t \psi(\tau, u(\tau)) \text{d}\tau\) with \(\phi(t_n) = 0\) and construct the ODE
        \begin{equation}\label{eq_extendedODE}
            \frac{\text{d}}{\text{d}t}
            \begin{bmatrix}
                \phi(t)\\
                u(t)
            \end{bmatrix} =
            \begin{bmatrix}
                \psi(t, u(t))\\
                f(t,u(t))
            \end{bmatrix}, t\in(t_n, T), \quad
            \begin{bmatrix}
                \phi(t_n)\\
                u(t_n)
            \end{bmatrix}=
            \begin{bmatrix}
                0\\
                u^n
            \end{bmatrix}
        \end{equation}

    Applying a \(p\)th order accurate Runge-Kutta method to Equation~\ref{eq_extendedODE} we get
    \[
    \begin{bmatrix}
        \phi^{n+1}\\
        u^{n+1}
    \end{bmatrix} =
    \begin{bmatrix}
        \phi^{n}\\
        u^{n}
    \end{bmatrix} +
    \begin{bmatrix}
        \Delta t \sum_{i=1}^{s}b_i\psi(t_n + c_i \Delta t, y_i)\\
        \Delta t \sum_{i=1}^{s}b_if(t_n + c_i \Delta t, y_i)
    \end{bmatrix}
        \]
    Since \(\phi^n=0\) we get that
    \begin{equation}\label{Eq_psi_proof_1}
        \Delta t \sum_{i=1}^{s}b_i\psi(t_n + c_i \Delta t, y_i) = \phi^{n+1} = \phi(t_n + \Delta t) + \mathcal{O}(\Delta t^{p+1})
    \end{equation}

    We also have that
    \begin{align}
        \phi(t_n + \Delta t) &= \int_{t_n}^{t_n + \Delta t} \psi(t, u(t))\text{d}t \nonumber \\
        &= \Delta t \sum_{i=1}^{s}b_i\psi(t_n + c_i\Delta t, u(t_n + c_i \Delta t)) + \mathcal{O}(\Delta t^{p+1}) \label{Eq_psi_proof_2}
    \end{align}

    Equations~\ref{Eq_psi_proof_1}~and~\ref{Eq_psi_proof_2} together gives
    \[\sum_{i=1}^{s}b_i\psi(t_n + c_i \Delta t, y_i) = \sum_{i=1}^{s}b_i\psi(t_n + c_i\Delta t, u(t_n + c_i \Delta t)) + \mathcal{O}(\Delta t^{p+1}).\]

    \end{proof}

    From Theorem~\ref{Th_psi} we get the following corollary.
    \begin{corollary}\label{Corr_gamma}
        If \(\eta\) is smooth and the given Runge-Kutta method is \(p\)-order accurate, \(r(\gamma = 1) = \mathcal{O}(\Delta t^{p+1})\).
    \end{corollary}
    \begin{proof}
        Inserting \(\phi(t,u) = \inner{\eta'(u)}{f(t,u)}\) into Theorem~\ref{Th_psi} and applying Equation~\ref{Eq_for_Corr} we get
        \[\eta(u^{n+1}) - \eta(u^n) = \Delta t \sum_{i=1}^{s} b_i \inner{\eta'(y_i)}{f(t_n + c_i\Delta t, y_i)} + \mathcal{O}(\Delta t^{p+1}),\]
        or equivalently
        \[r(\gamma = 1) = \mathcal{O}(\Delta t^{p+1}).\]
    \end{proof}

    \vspace*{5mm}
    With these results we can now construct the main theorem.
    \begin{theorem}\label{Th_main}
        Assume there exists a positive root \(\gamma_n\) of \(r\). Consider the IDT/RRK methods based on a given Runge-Kutta method that is \(p\)-order accurate. Then
        \begin{enumerate}
            \item The IDT method interpreting \(u_{\gamma}^{n+1} \approx u(t_n + \Delta t)\) has order \(p-1\).
            \item The RRK method interpreting \(u_{\gamma}^{n+1} \approx u(t_n + \gamma \Delta t)\) has order \(p\).
        \end{enumerate}
    \end{theorem}
    \begin{proof}
        We take \(1+k\) to be a root of \(r\) with \(k<\Delta t \in \mathbb{R}\). By the Taylor expansion
        \[r(1+k) = r(1) + r'(1)k + \mathcal{O}(k^2)\]
        We have from Corollary~\ref{Corr_gamma} that \(r(1) = \mathcal{O}(\Delta t^{p+1})\) and from Lemma~\ref{Lem_rp1} that \(r'(1)=c\Delta t^2 + \mathcal{O}(\Delta t^{3})\) for some \(c \in \mathbb{R^+}\). Inserting this we get

        \[0 = \mathcal{O}(\Delta t^{p+1}) + (c\Delta t^2 + \mathcal{O}(\Delta t^{3})) k + \mathcal{O}(k^2).\]

        Which is equivalent to

        \[k = \frac{\mathcal{O}(\Delta t^{p+1})}{c\Delta t^2 + \mathcal{O}(\Delta t^{3})} = \mathcal{O}(\Delta t^{p-1})\]

        Thus there is a root \(\gamma_n = 1 + \mathcal{O}(\Delta t^{p-1})\) of \(r\). Applying to Theorem~\ref{Th_Ketcheson} yields the desired result.
    \end{proof}


\section{Numerical Examples}

    Since the stepsize of RRK is unknown a priori, the last step of the method is always made using the ordinary RK method. This was applied to the IDT methods as well, although not necessary.
    Because of this we are in our relevant examples regarding the second to last value instead of the last.

    \vspace*{5mm}
    We are studying the effects of values of \(\Delta t\) close to \(1\) in the examples. These are not reasonable choices for \(\Delta t\), as the roots of \(r\) become less predictable. For the scalar root finding the method \texttt{scipy.optimize.root\_scalar} was used within the hardcoded range \( \left[0.5,2\right] \) was used.

    \vspace*{5mm}
    Note that fewer RK-methods were used for Problem 1 than in Problem 2. This is because the higher order Runge-Kutta methods resulted in an error from the method \texttt{isClose} used somewhere in \texttt{Assimulo}, a library of solvers that ours were written to inherit from.

    \vspace*{5mm}
    The problems presented in this section are two of the same used in the article~\cite{Ranocha_2020}, with the only change being that the source had swapped the first and second position of the analytic solution.

    \vspace*{5mm}
    The Butcher tableaus for SSPRK(3,3) and RK(4,4) were sourced online from~\cite{wikipedia_RK} while the ones for VRK(9,6) and VRK(13,8) were taken from~\cite{VRK96} and~\cite{VRK138} respectively.

\subsection{Problem 1 - Conserved exponential entropy}
    For this problem we consider the system

    \[\frac{\text{d}}{\text{d}t}
    \begin{bmatrix}
        u_1(t)\\
        u_2(t)
    \end{bmatrix} =
    \begin{bmatrix}
        -\expalt(u_2(t))\\
        \expalt(u_1(t))
    \end{bmatrix}, \qquad u^0 =
    \begin{bmatrix}
        1\\
        0.5
    \end{bmatrix},
    \]
    with exponential entropy
    \[\eta(u) = \expalt(u_1) + \expalt(u_2), \qquad \eta'(u) =
    \begin{bmatrix}
        \expalt(u_1)\\
        \expalt(u_2)
    \end{bmatrix},
    \]
    and analytic solution
    \[u(t) = \left(\log\left(\frac{\exp{} + \exp{3/2}}{\sqrt{\exp{}} + \exp{(\sqrt{\exp{}}+\exp{})t}}\right), \log\left(\frac{\exp{(\sqrt{\exp{}}+\exp{})t}(\sqrt{\exp{}}+\exp{})}{\sqrt{\exp{}} + \exp{(\sqrt{\exp{}}+\exp{})t}}\right)\right)^{\T}.\]

    \begin{figure}[H]
        \centering
        \subfloat[\(r(\gamma)\) for SSPRK(3,3)\label{Fig1a}]{\includegraphics[width=0.50\textwidth]{figs/P1_r_of_gamma.pdf}}\hfill
        \subfloat[\(\left|r(\gamma = 1)\right|\) for some RK methods\label{Fig1b}] {\includegraphics[width=0.50\textwidth]{figs/P1_r_at_1.pdf}}\hfill
        \caption{Numerical results for \(r\) at the first time step of problem 1.} \label{Fig1}
    \end{figure}

    From Figure~\ref{Fig1a} we can see that the shape of \(r(\gamma)\) behaves as expected, with roots at \(0\) and around \(1\).
    In Figure~\ref{Fig1b} we see that \(r(\gamma = 1) = \mathcal{O}(\Delta t^{p+1})\) as predicted in Corollary~\ref{Corr_gamma}.

    \begin{figure}[H]
        \centering
        \subfloat[Classic methods\label{Fig2a}]{\includegraphics[width=0.5\textwidth]{figs/P1_Regular_RK4.pdf}}\hfill
        \subfloat[RRK Methods\label{Fig2b}] {\includegraphics[width=0.5\textwidth]{figs/P1_Relaxed_RK4.pdf}}\hfill\\
        \subfloat[IDT methods\label{Fig2c}] {\includegraphics[width=0.5\textwidth]{figs/P1_IDT_RK4.pdf}}\hfill
        \caption{Convergence study for problem 1.} \label{Fig2}
    \end{figure}

    From Figure~\ref{Fig2b} we see that the RRK methods are of order \(p\) just like the unmodified methods in Figure~\ref{Fig2a}, meanwhile the IDT methods pictured in Figure~\ref{Fig2c} are of order \(p-1\), all in accordance with Theorem~\ref{Th_main}.

    \begin{figure}[H]
        \centering
        \subfloat[SSPK(3,3)\label{Fig10a}]{\includegraphics[width=0.5\textwidth]{figs/P1_eta_error_SSPRK33.pdf}}\hfill
        \subfloat[RK(4,4)\label{Fig10b}] {\includegraphics[width=0.5\textwidth]{figs/P1_eta_error_RK44.pdf}}\hfill
        \caption{Error of entropy for problem 1.}\label{Fig10}
    \end{figure}

    In Figures~\ref{Fig10a}~and~\ref{Fig10b} we see the error of the entropy \(\eta\) in the approximated points compared to the entropy of the analytic solution. Both RRK and IDT seem to perform equally well being zero withing floating point accuracy, which is considerably better than the unmodified method.

\subsection{Problem 2 - Dissipated exponential entropy}
    Consider the ODE
    \[\frac{\text{d}}{\text{d}t} u(t) = -\expalt(u(t)), \qquad u^0 = 0.5,\]
    with the exponential entropy
    \[\eta(u) = \expalt(u), \qquad \eta'(u) = \expalt(u),\]
    and analytical solution
    \[u(t) = -\log(\exp{-1/2}+t).\]

    \begin{figure}[H]
        \centering
        \subfloat[\(r(\gamma)\) for SSPRK(3,3)\label{Fig3a}]{\includegraphics[width=0.48\textwidth]{figs/P2_r_of_gamma.pdf}}\hfill
        \subfloat[\(\left|r(\gamma = 1)\right|\) for some RK methods\label{Fig3b}] {\includegraphics[width=0.48\textwidth]{figs/P2_r_at_1.pdf}}\hfill
        \caption{Numerical results for \(r\) at the first time step of problem 2.} \label{Fig3}
    \end{figure}

    From Figures~\ref{Fig3a}~and~\ref{Fig3b} we once more see the expected behavior of \(r\) and its roots.

    \begin{figure}[H]
        \centering
        \subfloat[Classic methods\label{Fig4a}]{\includegraphics[width=0.5\textwidth]{figs/P2_Regular_RK4.pdf}}\hfill
        \subfloat[RRK Methods\label{Fig4b}] {\includegraphics[width=0.5\textwidth]{figs/P2_Relaxed_RK4.pdf}}\hfill\\
        \subfloat[IDT methods\label{Fig4c}] {\includegraphics[width=0.5\textwidth]{figs/P2_IDT_RK4.pdf}}\hfill
        \caption{Convergence study for problem 2.} \label{Fig4}
    \end{figure}

    Once more, the behavior of the methods are as proposed in Theorem~\ref{Th_main}.

    % \begin{figure}[H]
    %     \centering
    %     \includegraphics[width=0.95\textwidth]{figs/P2_Regular_RK4.pdf}
    %     \caption{Convergence study for problem 2, classic methods.}
    %     \label{Fig4a}
    % \end{figure}

    % \begin{figure}[H]
    %     \centering
    %     \includegraphics[width=0.95\textwidth]{figs/P2_Relaxed_RK4.pdf}
    %     \caption{Convergence study for problem 2, relaxed methods.}
    %     \label{Fig4b}
    % \end{figure}

    % \begin{figure}[H]
    %     \centering
    %     \includegraphics[width=0.95\textwidth]{figs/P2_IDT_RK4.pdf}
    %     \caption{Convergence study for problem 2, IDT methods.}
    %     \label{Fig4c}
    % \end{figure}

    \begin{figure}[H]
        \centering
        \subfloat[SSPK(3,3)\label{Fig20a}]{\includegraphics[width=0.5\textwidth]{figs/P2_eta_error_SSPRK33.pdf}}\hfill
        \subfloat[RK(4,4)\label{Fig20b}] {\includegraphics[width=0.5\textwidth]{figs/P2_eta_error_RK44.pdf}}\hfill\\
        \subfloat[VRK(9,6)\label{Fig20c}]{\includegraphics[width=0.5\textwidth]{figs/P2_eta_error_VRK96.pdf}}\hfill
        \subfloat[VRK(13,8)\label{Fig20d}] {\includegraphics[width=0.5\textwidth]{figs/P2_eta_error_VRK138.pdf}}\hfill
        \caption{Error of entropy for problem 2.}\label{Fig20}
    \end{figure}

    When we compare the errors of entropy for problem 2 we get the strange result that in Figures~\ref{Fig20a}~and~\ref{Fig20b} the unmodified methods seem to ourperform RRK and both outperform IDT. In Figures~\ref{Fig20c}~and~\ref{Fig20d} the RRK and the unmodified methods perform similarly being zero withing floating point accuracy, and once again the are both better than the IDT methods.

\section{Conclusion}
    % \begin{itemize}
    %     \item Simple
    %     \item Cheap
    %     \item Physically correct
    %     \item Requires \(\eta\)
    % \end{itemize}
As we have seen the method is simple to set up, being just a few additional lines of code added to the classic and well known Runge-Kutta method.
Furthermore, as the most computation heavy part of it is a scalar root finding problem, it is cheap to perform.
Thus, since the entropy stability makes the model physically correct there is no reason not to implement it.
One drawback is that the method requires prior knowledge of \(\eta\). This is however a reasonable assumption as someone interested in conserving the entropy of their simulation should have some insight into the entropy of their problem.
A practice, an adaptation in code could take the entropy as an optional argument and when not supplied default into the classic Runge-Kutta. This requires little effort from the coder and leaves the decision in the hands of the user.
% However as a fallback a choice of \(\gamma_n\) satisfying
% \[ \frac{1}{2} \norm{u^{n+1}_{\gamma}}^2 - \frac{1}{2}\norm{u^{n}}^2 = \gamma_n \Delta t \sum_{i=1}^{s}b_i\inner{y_i}{f_i}\]
% as suggested by Ketcheson in \cite{ketcheson2019relaxation} could be used.

\bibliography{refs}{}
\bibliographystyle{ieeetr}

\end{document}